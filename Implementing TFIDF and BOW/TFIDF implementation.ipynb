{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from math import log10\n",
    "from sklearn.preprocessing import normalize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    \"\"\"\n",
    "    input is list of reviews and output is dictionary with value for each key as its index in unique_words\n",
    "    \"\"\"\n",
    "    unique_words =set() # creating a empty set of unique_words\n",
    "    if isinstance(dataset, (list,)): # if the input is a list\n",
    "        for row in dataset: # for each row in dataset\n",
    "            for word in row.split(): # tokenizing each word in document\n",
    "                if len(word) < 2: # we will consider a word if its length is greater tham 1\n",
    "                    continue\n",
    "                unique_words.add(word) # inserting the word in the set\n",
    "        unique_words = sorted(list(unique_words)) # sorting the list of unique words\n",
    "        vocab = {j:i for i, j in enumerate(unique_words)} # creating a dictionary with value for each key its index\n",
    "        return vocab # returning the vocab\n",
    "    else:\n",
    "        print(\"Enter the list of Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF(dataset,word):\n",
    "    \"\"\"\n",
    "    input is the list of document and word for which we the function will return the Document frequencny , i.e. the number of ducuments\n",
    "    in which the word occurs\n",
    "    \"\"\"\n",
    "    if isinstance(dataset,(list,)):\n",
    "        count= 0 # initializing the count with 0\n",
    "        for row in dataset: # for each row in the dataset\n",
    "            if word in row: # if the word in there in the row\n",
    "                count = count+1 # increment the count\n",
    "        return count\n",
    "    else:\n",
    "        print(\"Enter the list of documents\")\n",
    "def transform(dataset, vocab):\n",
    "    \"\"\"\n",
    "     dataset-> list of strings\n",
    "     vocab: dictionay with having unique words as key and the index as their count\n",
    "     return-> the sparse matrix\n",
    "    \"\"\"\n",
    "    columns = []  # creating a list to store the column index\n",
    "    rows = [] # creating a list to store the row inde\n",
    "    values = [] # creating the list to store the TFIDF value for each word in document\n",
    "    N = len(dataset) # storing the number of documents\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(dataset): # for each row in list\n",
    "            word_freq = dict(Counter(row.split())) # creating a dictionary of word frequency of each list\n",
    "            for word, freq in word_freq.items(): # for each item in the list\n",
    "                if len(word) <2 : # considering those items whose len is greater than 1\n",
    "                    continue\n",
    "                column_index = vocab.get(word, -1) # getting the column index for each word\n",
    "                if column_index!=-1: # if the word exist in the vocab\n",
    "                    df = DF(dataset,word) # getting the document frequency for each word\n",
    "                    tfidf = (freq/len(row.split()))*(1+np.log(1+N/1+df)) # calculating the TFIDF value for each word\n",
    "                    columns.append(column_index) # storing index for the word\n",
    "                    rows.append(idx) # storing the row number of the word\n",
    "                    values.append(tfidf) # appending the tfidf value of the word\n",
    "    return normalize(csr_matrix((values,(rows, columns)), shape=(len(dataset),len(vocab)))) # returing the nomalized sparse matrix\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse']\n",
      "[[0.         0.         0.         0.24715587 0.27026754 0.27026754\n",
      "  0.         0.24715587 0.24715587 0.24715587 0.27026754 0.24715587\n",
      "  0.         0.24715587 0.24715587 0.         0.         0.54053508\n",
      "  0.         0.         0.         0.24715587]\n",
      " [0.27128981 0.27128981 0.27128981 0.         0.         0.29665826\n",
      "  0.27128981 0.         0.         0.         0.29665826 0.\n",
      "  0.27128981 0.         0.         0.27128981 0.27128981 0.29665826\n",
      "  0.27128981 0.27128981 0.27128981 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit(strings)\n",
    "print(list(vocab.keys()))\n",
    "print(transform(strings, vocab).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results with TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'centerpiece', 'economic', 'economists', 'for', 'is', 'its', 'lagrange', 'method', 'multipliers', 'of', 'optimization', 'poorly', 'problems', 'solving', 'taught', 'technique', 'the', 'theory', 'unfortunately', 'usually', 'workhorse']\n",
      "[[0.         0.         0.         0.28822557 0.28822557 0.20507486\n",
      "  0.         0.28822557 0.28822557 0.28822557 0.20507486 0.28822557\n",
      "  0.         0.28822557 0.28822557 0.         0.         0.41014973\n",
      "  0.         0.         0.         0.28822557]\n",
      " [0.29464404 0.29464404 0.29464404 0.         0.         0.20964166\n",
      "  0.29464404 0.         0.         0.         0.20964166 0.\n",
      "  0.29464404 0.         0.         0.29464404 0.29464404 0.20964166\n",
      "  0.29464404 0.29464404 0.29464404 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "new_string = tf.fit_transform(strings)\n",
    "print(tf.get_feature_names())\n",
    "print(new_string.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider only n words with top IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_new(dataset, word):\n",
    "    \"\"\"\n",
    "    input-> dataset, word\n",
    "    dataset->list of sentences\n",
    "    word-> word for which i have to find the IDF value\n",
    "    \"\"\"\n",
    "    count = 0 # initialzing the coount with zero\n",
    "    for row in dataset: # for each row in the list\n",
    "        if word in row: # if the word is present in the list\n",
    "            count += 1 # increament the count\n",
    "    val =1+ np.log((1+len(dataset)/(count+1))) #calculating the IDF value\n",
    "    return val # returng the IDF value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def df_new(dataset, word):\n",
    "    count = 0\n",
    "    for row in dataset:\n",
    "        if word in dataset:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "def fit_new(dataset, n):\n",
    "    \"\"\"\n",
    "    input-> dataset ,n\n",
    "    dataset= list of strings\n",
    "    n- number of top words based on IDF values\n",
    "    return the vocab having top n words based on IDF values\n",
    "    \"\"\"\n",
    "    unique_words = set() # creating a set of unique words\n",
    "    tdf_values  = [] # creatng a list to store the values ofIDF for each word\n",
    "    for row in dataset: # for each row in the dataset\n",
    "        for word in row.split(): # for each word in the row\n",
    "            if len(word)<2: # we will consider only those words for which the len is geater than 1\n",
    "                continue\n",
    "            unique_words.add(word) # inserting the word in the set\n",
    "    unique_words = list(unique_words)\n",
    "    \n",
    "    #sorting the list of unique_words based on IDF values\n",
    "    unique_words= sorted(unique_words, key = lambda x: idf_new(dataset, x), reverse=True)[:n]\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    return vocab\n",
    "    \n",
    "    \n",
    "def transform_new(dataset, vocab):\n",
    "    \"\"\"\n",
    "     dataset-> list of strings\n",
    "     vocab: dictionay with having unique words as key and the index as their count\n",
    "     return-> the sparse matrix\n",
    "    \"\"\"\n",
    "    columns = []  # creating a list to store the column index\n",
    "    rows = [] # creating a list to store the row inde\n",
    "    values = [] # creating the list to store the TFIDF value for each word in document\n",
    "    N = len(dataset) # storing the number of documents\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(dataset): # for each row in list\n",
    "            word_freq = dict(Counter(row.split())) # creating a dictionary of word frequency of each list\n",
    "            for word, freq in word_freq.items(): # for each item in the list\n",
    "                if len(word) <2 : # considering those items whose len is greater than 1\n",
    "                    continue\n",
    "                column_index = vocab.get(word, -1) # getting the column index for each word\n",
    "                if column_index!=-1: # if the word exist in the vocab\n",
    "                    df = df_new(dataset,word) # getting the document frequency for each word\n",
    "                    tfidf = (freq/len(row.split()))*(1+np.log(1+N/1+df)) # calculating the TFIDF value for each word\n",
    "                    columns.append(column_index) # storing index for the word\n",
    "                    rows.append(idx) # storing the row number of the word\n",
    "                    values.append(tfidf) # appending the tfidf value of the word\n",
    "    return normalize(csr_matrix((values,(rows, columns)), shape=(len(dataset),len(vocab)))) # returing the nomalized sparse matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solving', 'method', 'economists', 'poorly', 'centerpiece', 'unfortunately', 'taught', 'workhorse', 'its', 'but']\n",
      "[[0.5        0.5        0.5        0.         0.         0.\n",
      "  0.         0.5        0.         0.        ]\n",
      " [0.         0.         0.         0.40824829 0.40824829 0.40824829\n",
      "  0.40824829 0.         0.40824829 0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit_new(strings,10)\n",
    "print(list(vocab.keys()))\n",
    "print(transform_new(strings, vocab).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
